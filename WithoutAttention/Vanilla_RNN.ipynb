{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "xbhcprcU3gcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "import wandb\n",
        "wandb.login(key='1ee7845713d1303ac1abff70cf959518e1ae311c')\n",
        "wandb.init(project=\"cs6910_RNN\")\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import csv\n",
        "import heapq\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"ticks\", context=\"talk\")\n",
        "plt.style.use(\"dark_background\")\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:13.312871Z",
          "iopub.execute_input": "2023-05-22T18:44:13.313262Z",
          "iopub.status.idle": "2023-05-22T18:44:23.813298Z",
          "shell.execute_reply.started": "2023-05-22T18:44:13.313229Z",
          "shell.execute_reply": "2023-05-22T18:44:23.812308Z"
        },
        "trusted": true,
        "id": "7mfB_TEo3W0t"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:26.457004Z",
          "iopub.execute_input": "2023-05-22T18:44:26.457680Z",
          "iopub.status.idle": "2023-05-22T18:44:26.463105Z",
          "shell.execute_reply.started": "2023-05-22T18:44:26.457650Z",
          "shell.execute_reply": "2023-05-22T18:44:26.462200Z"
        },
        "trusted": true,
        "id": "2SNOqvg13W0v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOADING THE DATA and PREPROCESSING"
      ],
      "metadata": {
        "id": "Fzs_Xj_iYfNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self,filepath):\n",
        "        self.X=[]\n",
        "        self.Y=[]\n",
        "        self.input_corpus= set()\n",
        "        self.output_corpus= set()\n",
        "        csv_file=open(filepath)\n",
        "        my_csv=csv.reader(csv_file)\n",
        "        for i in my_csv:   \n",
        "            self.X.append(i[0])\n",
        "            self.Y.append(i[1])\n",
        "        self.X = np.array(self.X)\n",
        "        self.Y = np.array(self.Y)\n",
        "\n",
        "    def add_start_end_tokens(self):\n",
        "        for i in range(self.Y.shape[0]):\n",
        "            self.Y[i] = \"\\t\" + self.Y[i] + \"\\n\"\n",
        "        return self.X,self.Y\n",
        "\n",
        "    def get_input_corpus(self):\n",
        "        for word in self.X:\n",
        "            for char in word:\n",
        "                if char not in self.input_corpus:\n",
        "                    self.input_corpus.add(char)\n",
        "\n",
        "        self.input_corpus.add(\" \")\n",
        "        self.input_corpus = sorted(list(self.input_corpus))\n",
        "        return self.input_corpus\n",
        "\n",
        "    def get_output_corpus(self):\n",
        "        for word in self.Y:\n",
        "            for char in word:\n",
        "                if char not in self.output_corpus:\n",
        "                    self.output_corpus.add(char)\n",
        "        self.output_corpus.add(\" \")\n",
        "        self.output_corpus = sorted(list(self.output_corpus))\n",
        "        return self.output_corpus\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:27.825802Z",
          "iopub.execute_input": "2023-05-22T18:44:27.826552Z",
          "iopub.status.idle": "2023-05-22T18:44:27.845418Z",
          "shell.execute_reply.started": "2023-05-22T18:44:27.826515Z",
          "shell.execute_reply": "2023-05-22T18:44:27.844447Z"
        },
        "trusted": true,
        "id": "k__6wLR_3W0v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=Dataset(\"/kaggle/input/a3data/hin_train.csv\") \n",
        "train_X,train_Y=train_data.add_start_end_tokens()\n",
        "\n",
        "valid_data=Dataset(\"/kaggle/input/a3data/hin_valid.csv\")\n",
        "val_X,val_Y=valid_data.add_start_end_tokens()\n",
        "\n",
        "test_data=Dataset(\"/kaggle/input/a3data/hin_test.csv\")\n",
        "test_X,test_Y=test_data.add_start_end_tokens()\n",
        "\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:28.860968Z",
          "iopub.execute_input": "2023-05-22T18:44:28.863519Z",
          "iopub.status.idle": "2023-05-22T18:44:29.082273Z",
          "shell.execute_reply.started": "2023-05-22T18:44:28.863484Z",
          "shell.execute_reply": "2023-05-22T18:44:29.081358Z"
        },
        "trusted": true,
        "id": "7ohQaumV3W0v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_corpus = train_data.get_input_corpus()\n",
        "output_corpus = train_data.get_output_corpus()\n",
        "\n",
        "num_encoder_tokens = len(input_corpus)\n",
        "num_decoder_tokens = len(output_corpus)\n",
        "#The lengths represent the count of unique characters in the input(encoder) and output sequences(decoder),\n",
        "#serving as the vocabulary size for each part.\n",
        "print(input_corpus)\n",
        "print(output_corpus)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:29.623602Z",
          "iopub.execute_input": "2023-05-22T18:44:29.624021Z",
          "iopub.status.idle": "2023-05-22T18:44:29.631692Z",
          "shell.execute_reply.started": "2023-05-22T18:44:29.623989Z",
          "shell.execute_reply": "2023-05-22T18:44:29.630723Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3BKyD823W0w",
        "outputId": "43a3d3e4-c11c-41d2-b4a1-dcfb77854b8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "['\\t', '\\n', ' ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_encoder_seq_length = max([len(txt) for txt in train_X]) \n",
        "max_decoder_seq_length = max([len(txt) for txt in train_Y])+2\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:29.633143Z",
          "iopub.execute_input": "2023-05-22T18:44:29.633822Z",
          "iopub.status.idle": "2023-05-22T18:44:29.699366Z",
          "shell.execute_reply.started": "2023-05-22T18:44:29.633790Z",
          "shell.execute_reply": "2023-05-22T18:44:29.698506Z"
        },
        "trusted": true,
        "id": "V9lgdQIo3W0w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the input character index dictionary\n",
        "input_char_index = {}\n",
        "for i, char in enumerate(input_corpus):\n",
        "    input_char_index[char] = i\n",
        "\n",
        "# Create the output character index dictionary\n",
        "output_char_index = {}\n",
        "for i, char in enumerate(output_corpus):\n",
        "    output_char_index[char] = i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def populate_data_arrays(data_X, data_Y, input_char_index, output_char_index, max_encoder_seq_length, max_decoder_seq_length):\n",
        "    num_samples = len(data_X)\n",
        "    input_data = np.zeros((max_encoder_seq_length + 1, num_samples), dtype=\"int64\")\n",
        "    target_data = np.zeros((max_decoder_seq_length + 1, num_samples), dtype=\"int64\")\n",
        "    \n",
        "    for i, (x, y) in enumerate(zip(data_X, data_Y)):\n",
        "        for t, char in enumerate(x):\n",
        "            input_data[t, i] = input_char_index[char]\n",
        "        input_data[t + 1:, i] = input_char_index[\" \"]\n",
        "        \n",
        "        for t, char in enumerate(y):\n",
        "            target_data[t, i] = output_char_index[char]\n",
        "        target_data[t + 1:, i] = output_char_index[\" \"]\n",
        "    \n",
        "    return input_data, target_data\n",
        "\n",
        "# Usage:\n",
        "input_data, target_data = populate_data_arrays(train_X, train_Y, input_char_index, output_char_index,\n",
        "                                               max_encoder_seq_length, max_decoder_seq_length)\n",
        "input_data_val, target_data_val = populate_data_arrays(val_X, val_Y, input_char_index, output_char_index,\n",
        "                                                       max_encoder_seq_length, max_decoder_seq_length)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:29.719390Z",
          "iopub.execute_input": "2023-05-22T18:44:29.719822Z",
          "iopub.status.idle": "2023-05-22T18:44:30.502291Z",
          "shell.execute_reply.started": "2023-05-22T18:44:29.719790Z",
          "shell.execute_reply": "2023-05-22T18:44:30.501304Z"
        },
        "trusted": true,
        "id": "OI1Z8GVy3W0x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertin numpy arrays to tensors\n",
        "input_data = torch.tensor(input_data,dtype=torch.int64)\n",
        "target_data = torch.tensor(target_data,dtype=torch.int64)\n",
        "input_data_val = torch.tensor(input_data_val,dtype=torch.int64)\n",
        "target_data_val = torch.tensor(target_data_val,dtype=torch.int64)\n",
        "print(input_data.shape,target_data.shape,input_data_val.shape,target_data_val.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.503700Z",
          "iopub.execute_input": "2023-05-22T18:44:30.504029Z",
          "iopub.status.idle": "2023-05-22T18:44:30.553442Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.503997Z",
          "shell.execute_reply": "2023-05-22T18:44:30.552604Z"
        },
        "trusted": true,
        "id": "JU9LnH-E3W0x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the reverse input character index dictionary\n",
        "reverse_input_char_index = {}\n",
        "for char, i in input_char_index.items():\n",
        "    reverse_input_char_index[i] = char\n",
        "\n",
        "# Create the reverse output character index dictionary\n",
        "reverse_target_char_index = {}\n",
        "for char, i in output_char_index.items():\n",
        "    reverse_target_char_index[i] = char"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.554872Z",
          "iopub.execute_input": "2023-05-22T18:44:30.555241Z",
          "iopub.status.idle": "2023-05-22T18:44:30.560457Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.555210Z",
          "shell.execute_reply": "2023-05-22T18:44:30.559434Z"
        },
        "trusted": true,
        "id": "B69UOgIy3W0x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "l5WWUp2r3mYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enoder LSTM Class\n",
        "class EncoderLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    A LSTM encoder for sequence-to-sequence learning.\n",
        "    Args:\n",
        "        inp_vocab_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The size of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state.\n",
        "        num_layers (int): The number of GRU layers.\n",
        "        p (float): The dropout rate.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, inp_vocab_size, embedding_size, hidden_size, num_layers, d):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        # Store the input parameters        \n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(d)\n",
        "        self.inp_vocab_size = inp_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define the embedding layer\n",
        "        self.cell_type=\"LSTM\"\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=d)\n",
        "\n",
        "    def forward(self, inpx):\n",
        "        \"\"\"\n",
        "        Encodes a sequence of input tokens.\n",
        "\n",
        "        Args:\n",
        "        x (torch.Tensor): A tensor of shape (seq_length, batch_size).\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The hidden state of the GRU, of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "        \"\"\"\n",
        "        # Apply dropout to the input\n",
        "        embedded_inp=self.embedding(inpx)\n",
        "        embedded_inp_drop = self.dropout(embedded_inp)\n",
        "\n",
        "        # Pass the embedded input through the LSTM layer\n",
        "        outputs, (hiddenstate, cellstate) = self.rnn(embedded_inp_drop)\n",
        "\n",
        "        return hiddenstate, cellstate\n",
        "\n",
        "\n",
        "\n",
        "#Deoder LSTM class\n",
        "class DecoderLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    A LSTM decoder for sequence-to-sequence learning.\n",
        "\n",
        "    Args:\n",
        "        inp_vocab_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The size of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state.\n",
        "        output_size (int): The size of the output vocabulary.\n",
        "        num_layers (int): The number of GRU layers.\n",
        "        p (float): The dropout rate.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, inp_vocab_size, embedding_size, hidden_size, output_size, num_layers, d):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        # Store the input parameters\n",
        "        self.inp_vocab_size = inp_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(d)\n",
        "\n",
        "        # Define the embedding layer\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=d)\n",
        "\n",
        "        # Define the fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self,inpx , hidden, cell):\n",
        "        \"\"\"\n",
        "        Decodes a sequence of input tokens and generates a sequence of output tokens.\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (1, batch_size).\n",
        "            hidden (torch.Tensor): The hidden state of the GRU, of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (seq_length, batch_size, output_size).\n",
        "\n",
        "        \"\"\"\n",
        "        # x shape: (N) where N is batch size, we want it to be (1, N)\n",
        "        inpx = inpx.unsqueeze(0)\n",
        "        # Apply dropout to the input\n",
        "        \n",
        "        embedded_inp=self.embedding(inpx)\n",
        "        embedded_inp_drop = self.dropout(embedded_inp)\n",
        "        # embedded shape: (1, N, embedding_size)\n",
        "\n",
        "        # Pass the embedded input through the LSTM layer\n",
        "        outputs, (hiddenstate, cellstate) = self.rnn(embedded_inp_drop, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        # Pass the LSTM outputs through the fully connected layer\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # Reshape the predictions to match the target vocabulary size\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hiddenstate, cellstate\n",
        "\n",
        "\n",
        "\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    A sequence-to-sequence model with a GRU encoder and decoder.\n",
        "    Args:\n",
        "        encoder (EncoderGRU): The encoder.\n",
        "        decoder (DecoderGRU): The decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqLSTM, self).__init__()\n",
        "\n",
        "        # Store the encoder and decoder modules\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Performs forward propagation on the model.\n",
        "\n",
        "        Args:\n",
        "            source (torch.Tensor): A tensor of shape (seq_length, batch_size).\n",
        "            target (torch.Tensor): A tensor of shape (target_len, batch_size).\n",
        "            teacher_force_ratio (float): The probability of using teacher forcing.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (target_len, batch_size, output_size).\n",
        "\n",
        "        \"\"\"\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = num_decoder_tokens\n",
        "\n",
        "        # Initialize the outputs tensor\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        hidden_state, cell_state = self.encoder(source)\n",
        "\n",
        "        # Initialize the input to the decoder with the start token\n",
        "        input_token = target[0]\n",
        "\n",
        "        # Initialize the counter variable\n",
        "        t = 1\n",
        "\n",
        "        # Iterate over the target sequence length\n",
        "        while t < target_len:\n",
        "            # Pass the input, hidden state, and cell state through the decoder\n",
        "            decoder_output, hidden_state, cell_state = self.decoder(input_token, hidden_state, cell_state)\n",
        "\n",
        "            # Store the decoder output in the outputs tensor\n",
        "            outputs[t] = decoder_output\n",
        "\n",
        "            # Determine the next input to the decoder using teacher forcing\n",
        "            if random.random() < teacher_force_ratio:\n",
        "                input_token = target[t]\n",
        "            else:\n",
        "                input_token = decoder_output.argmax(1)\n",
        "\n",
        "            # Increment the counter\n",
        "            t += 1\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.562274Z",
          "iopub.execute_input": "2023-05-22T18:44:30.562614Z",
          "iopub.status.idle": "2023-05-22T18:44:30.579344Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.562563Z",
          "shell.execute_reply": "2023-05-22T18:44:30.578485Z"
        },
        "trusted": true,
        "id": "2JXt6sWv3W0x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRU MODEL"
      ],
      "metadata": {
        "id": "SFe-ev9DakHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EncoderGRU class\n",
        "class EncoderGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    A GRU encoder for sequence-to-sequence learning.\n",
        "\n",
        "    Args:\n",
        "        inp_vocab_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The size of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state.\n",
        "        num_layers (int): The number of GRU layers.\n",
        "        p (float): The dropout rate.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inp_vocab_size, embedding_size, hidden_size, num_layers, p=0.3):\n",
        "        super(EncoderGRU, self).__init__()\n",
        "\n",
        "        # Dropout layer.\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        # Embedding layer.\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "\n",
        "        # GRU layer.\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, inpx):\n",
        "        \"\"\"\n",
        "        Encodes a sequence of input tokens.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (seq_length, batch_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The hidden state of the GRU, of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Embedding layer.\n",
        "        embedded_inpx=self.embedding(inpx)\n",
        "        embedded_inpx_drop = self.dropout(embedded_inpx)\n",
        "\n",
        "        # GRU layer.\n",
        "        outputs, hidden = self.rnn(embedded_inpx_drop)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n",
        "# DecoderGRU class\n",
        "class DecoderGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    A GRU decoder for sequence-to-sequence learning.\n",
        "\n",
        "    Args:\n",
        "        inp_vocab_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The size of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state.\n",
        "        output_size (int): The size of the output vocabulary.\n",
        "        num_layers (int): The number of GRU layers.\n",
        "        p (float): The dropout rate.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inp_vocab_size, embedding_size, hidden_size, output_size, num_layers, p=0.3):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "\n",
        "        # Dropout layer.\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        # Embedding layer.\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "\n",
        "        # GRU layer.\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "        # Linear layer.\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Decodes a sequence of input tokens and generates a sequence of output tokens.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (1, batch_size).\n",
        "            hidden (torch.Tensor): The hidden state of the GRU, of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (seq_length, batch_size, output_size).\n",
        "\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(0)\n",
        "        # Embedding layer.\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        # GRU layer.\n",
        "        outputs, hidden = self.rnn(embedding, hidden)\n",
        "\n",
        "        # Linear layer.\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden\n",
        "\n",
        "\n",
        "# Seq2SeqGRU class\n",
        "class Seq2SeqGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    A sequence-to-sequence model with a GRU encoder and decoder.\n",
        "\n",
        "    Args:\n",
        "        encoder (EncoderGRU): The encoder.\n",
        "        decoder (DecoderGRU): The decoder.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqGRU, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Performs forward propagation on the model.\n",
        "\n",
        "        Args:\n",
        "            source (torch.Tensor): A tensor of shape (seq_length, batch_size).\n",
        "            target (torch.Tensor): A tensor of shape (target_len, batch_size).\n",
        "            teacher_force_ratio (float): The probability of using teacher forcing.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (target_len, batch_size, output_size).\n",
        "\n",
        "        \"\"\"\n",
        "        # Encode the source sequence.\n",
        "        hidden = self.encoder(source)\n",
        "        # Initialize the decoder hidden state.\n",
        "        decoder_hidden = hidden\n",
        "\n",
        "\n",
        "        # Initialize the output predictions.\n",
        "        predictions = torch.zeros(target.shape[0], source.shape[1],num_decoder_tokens ).to(device)\n",
        "\n",
        "        # Initialize the counter variable.\n",
        "        t = 0\n",
        "\n",
        "        x=target[0]\n",
        "        # Loop over the target sequence.\n",
        "        while t < target.shape[0]:\n",
        "\n",
        "            # Decode the next output token.\n",
        "            output, decoder_hidden = self.decoder(x, decoder_hidden)\n",
        "\n",
        "            # Store the output prediction.\n",
        "            predictions[t] = output\n",
        "\n",
        "            # Get the next input to the decoder.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else predictions[t].argmax(1)\n",
        "\n",
        "            # Increment the counter.\n",
        "            t += 1\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.582803Z",
          "iopub.execute_input": "2023-05-22T18:44:30.583125Z",
          "iopub.status.idle": "2023-05-22T18:44:30.598427Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.583073Z",
          "shell.execute_reply": "2023-05-22T18:44:30.597594Z"
        },
        "trusted": true,
        "id": "_UpFDdlA3W0y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN MODEL"
      ],
      "metadata": {
        "id": "7JatgOC3bek6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EncoderGRU class\n",
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A RNN encoder for sequence-to-sequence learning.\n",
        "\n",
        "    Args:\n",
        "        inp_vocab_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The size of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state.\n",
        "        num_layers (int): The number of RNN layers.\n",
        "        p (float): The dropout rate.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inp_vocab_size, embedding_size, hidden_size, num_layers, p=0.3):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Dropout layer.\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        # Embedding layer.\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "\n",
        "        # GRU layer.\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, inpx):\n",
        "        \"\"\"\n",
        "        Encodes a sequence of input tokens.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (seq_length, batch_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The hidden state of the RNN, of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Embedding layer.\n",
        "        embedded_inpx=self.embedding(inpx)\n",
        "        embedded_inpx_drop = self.dropout(embedded_inpx)\n",
        "\n",
        "        # GRU layer.\n",
        "        outputs, hidden = self.rnn(embedded_inpx_drop)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n",
        "# DecoderRNN class\n",
        "class DecoderRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A GRU decoder for sequence-to-sequence learning.\n",
        "\n",
        "    Args:\n",
        "        inp_vocab_size (int): The size of the input vocabulary.\n",
        "        embedding_size (int): The size of the embedding layer.\n",
        "        hidden_size (int): The size of the hidden state.\n",
        "        output_size (int): The size of the output vocabulary.\n",
        "        num_layers (int): The number of GRU layers.\n",
        "        p (float): The dropout rate.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inp_vocab_size, embedding_size, hidden_size, output_size, num_layers, p=0.3):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        # Dropout layer.\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        # Embedding layer.\n",
        "        self.embedding = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "\n",
        "        # GRU layer.\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "        # Linear layer.\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Decodes a sequence of input tokens and generates a sequence of output tokens.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): A tensor of shape (1, batch_size).\n",
        "            hidden (torch.Tensor): The hidden state of the RNN, of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (seq_length, batch_size, output_size).\n",
        "\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(0)\n",
        "        # Embedding layer.\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        # GRU layer.\n",
        "        outputs, hidden = self.rnn(embedding, hidden)\n",
        "\n",
        "        # Linear layer.\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden\n",
        "\n",
        "\n",
        "# Seq2SeqRNN class\n",
        "class Seq2SeqRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A sequence-to-sequence model with a RNN encoder and decoder.\n",
        "\n",
        "    Args:\n",
        "        encoder (EncoderGRU): The encoder.\n",
        "        decoder (DecoderGRU): The decoder.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqRNN, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Performs forward propagation on the model.\n",
        "\n",
        "        Args:\n",
        "            source (torch.Tensor): A tensor of shape (seq_length, batch_size).\n",
        "            target (torch.Tensor): A tensor of shape (target_len, batch_size).\n",
        "            teacher_force_ratio (float): The probability of using teacher forcing.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (target_len, batch_size, output_size).\n",
        "\n",
        "        \"\"\"\n",
        "        # Encode the source sequence.\n",
        "        hidden = self.encoder(source)\n",
        "        # Initialize the decoder hidden state.\n",
        "        decoder_hidden = hidden\n",
        "\n",
        "\n",
        "        # Initialize the output predictions.\n",
        "        predictions = torch.zeros(target.shape[0], source.shape[1],num_decoder_tokens ).to(device)\n",
        "\n",
        "        # Initialize the counter variable.\n",
        "        t = 0\n",
        "\n",
        "        x=target[0]\n",
        "        # Loop over the target sequence.\n",
        "        while t < target.shape[0]:\n",
        "\n",
        "            # Decode the next output token.\n",
        "            output, decoder_hidden = self.decoder(x, decoder_hidden)\n",
        "\n",
        "            # Store the output prediction.\n",
        "            predictions[t] = output\n",
        "\n",
        "            # Get the next input to the decoder.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else predictions[t].argmax(1)\n",
        "\n",
        "            # Increment the counter.\n",
        "            t += 1\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.599832Z",
          "iopub.execute_input": "2023-05-22T18:44:30.600217Z",
          "iopub.status.idle": "2023-05-22T18:44:30.613418Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.600186Z",
          "shell.execute_reply": "2023-05-22T18:44:30.612550Z"
        },
        "trusted": true,
        "id": "GMqrPDKc3W0y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation functions \n",
        "functions which takes a model , a input word and other required parameters and translates the word into target language"
      ],
      "metadata": {
        "id": "rbqt2sv0b6m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialiser(len):\n",
        "    return np.zeros((len,1),dtype=\"int64\")"
      ],
      "metadata": {
        "id": "nC5p42FscKgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_searchLSTM(model, word, input_char_idx, output_char_idx, reverse_input_char_idx,\n",
        "                reverse_target_char_idx, max_encoder_seq_len, max_decoder_seq_len,\n",
        "                encoder_tok_numbers, decoder_tok_numbers, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "    # Initialize the output word\n",
        "    word_t = ''\n",
        "\n",
        "    # Encode the input word\n",
        "    data = initialiser(max_encoder_seq_len)\n",
        "    t = 0\n",
        "    \n",
        "    while t < len(word):\n",
        "        char = word[t]\n",
        "        data[t, 0] = input_char_idx[char]\n",
        "        t += 1\n",
        "\n",
        "        \n",
        "    data[t :, 0] = input_char_idx[\" \"]\n",
        "    \n",
        "    \n",
        "    mydata_type=torch.int64\n",
        "    data = torch.tensor(data, dtype=mydata_type).to(device)\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    mychar=output_char_idx['\\t']\n",
        "    initial_sequence = torch.tensor(np.array(mychar).reshape(1,)).to(device)\n",
        "    initial_hidden=hidden.unsqueeze(0)\n",
        "    beam = [(0.0, initial_sequence,initial_hidden )]  # [(score, sequence, hidden)]\n",
        "\n",
        "    \n",
        "    time_step = 0\n",
        "    while time_step < max_decoder_seq_len:\n",
        "        candidates = []\n",
        "        idx = 0\n",
        "        while idx < len(beam):\n",
        "            score, seq, hidden = beam[idx]\n",
        "            last_token = seq[-1].item()\n",
        "            if last_token == output_char_idx['\\n']:\n",
        "                # If the sequence ends with the end token, add it to the candidates\n",
        "                candidates.append((score, seq, hidden))\n",
        "                idx += 1\n",
        "                continue\n",
        "\n",
        "            x = torch.tensor(np.array(last_token).reshape(1,)).to(device)\n",
        "            var=hidden.squeeze(0)\n",
        "            output, hidden, cell = model.decoder(x, var , cell)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "            # Get the top-k probabilities and tokens\n",
        "            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n",
        "\n",
        "            j = 0\n",
        "            while j < len(topk_probs[0]):\n",
        "                prob, token = topk_probs[0][j], topk_tokens[0][j]\n",
        "                \n",
        "                var2=token.unsqueeze(0)\n",
        "                new_seq = torch.cat((seq,var2 ), dim=0)\n",
        "                \n",
        "                new_hidden = hidden.clone().unsqueeze(0)\n",
        "                \n",
        "                length_penalty_factor = ((len(new_seq) - 1) / 5) ** length_penalty  # Adjust penalty factor as needed\n",
        "                myitem=torch.log(prob).item()\n",
        "                candidates.append((score + myitem / length_penalty_factor, new_seq, new_hidden))\n",
        "                j += 1\n",
        "\n",
        "            idx += 1\n",
        "\n",
        "        # Select top-k candidates based on the accumulated scores\n",
        "        #beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n",
        "        def get_score(item):\n",
        "            return item[0]\n",
        "\n",
        "        # Use heapq.nlargest to retrieve the top candidates based on score\n",
        "        beam = heapq.nlargest(beam_width, candidates, key=get_score)\n",
        "\n",
        "        time_step += 1\n",
        "\n",
        "    # Select the best sequence from the beam as the output\n",
        "    #best_score, best_sequence, tempvar = max(beam, key=lambda x: x[0])\n",
        "    \n",
        "    best_score = float('-inf')\n",
        "    best_sequence = None\n",
        "    tempvar = None\n",
        "    for item in beam:\n",
        "        score = item[0]\n",
        "        sequence = item[1]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_sequence = sequence\n",
        "            tempvar = item[2]\n",
        "    \n",
        "    \n",
        "    translated_word = ''\n",
        "    for token in best_sequence[1:-1]:\n",
        "        translated_word += reverse_target_char_idx[token.item()]\n",
        "\n",
        "    return translated_word\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.853723Z",
          "iopub.execute_input": "2023-05-22T18:44:30.853985Z",
          "iopub.status.idle": "2023-05-22T18:44:30.867898Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.853962Z",
          "shell.execute_reply": "2023-05-22T18:44:30.866946Z"
        },
        "trusted": true,
        "id": "Fgqnv-LB3W0z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_searchGRU_RNN(model, input_word, input_char_dict, output_char_dict, reverse_input_char_dict,\n",
        "                  reverse_output_char_dict, max_encoder_length, max_decoder_length,\n",
        "                  encoder_tok_numbers, decoder_tok_numbers, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "    decoded_word = ''\n",
        "\n",
        "    # Encode the input word\n",
        "    encoder_input = initialiser(max_encoder_length)\n",
        "\n",
        "    t = 0\n",
        "    while t < len(input_word):\n",
        "        encoder_input[t, 0] = input_char_dict[input_word[t]]\n",
        "        t += 1\n",
        "\n",
        "    encoder_input[t:, 0] = input_char_dict[\" \"]\n",
        "    \n",
        "    mydata_type=torch.int64\n",
        "    encoder_input = torch.tensor(encoder_input, dtype=mydata_type).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(encoder_input)\n",
        "\n",
        "    # Initialize beam\n",
        "    mychar=output_char_dict['\\t']\n",
        "    initial_sequence = torch.tensor(np.array(mychar).reshape(1,)).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n",
        "\n",
        "    time_step = 0\n",
        "    while time_step < max_decoder_length:\n",
        "        candidates = []\n",
        "        for score, seq, hidden in beam:\n",
        "            last_token = seq[-1].item()\n",
        "            if last_token == output_char_dict['\\n']:\n",
        "                # If the sequence ends with the end token, add it to the candidates\n",
        "                candidates.append((score, seq, hidden))\n",
        "                continue\n",
        "\n",
        "            x = torch.tensor(np.array(last_token).reshape(1,)).to(device)\n",
        "            initial_hidden=hidden.squeeze(0)\n",
        "            output, hidden = model.decoder(x, initial_hidden)\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "            # Get the top-k probabilities and tokens\n",
        "            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n",
        "\n",
        "            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "                unsqueeze_token=token.unsqueeze(0)\n",
        "                new_seq = torch.cat((  seq, unsqueeze_token   ), dim=0)\n",
        "                new_hidden = hidden.clone().unsqueeze(0)\n",
        "                length_penalty_factor = ((len(new_seq) - 1) / 5) ** length_penalty  # Adjust penalty factor as needed\n",
        "                myitem=torch.log(prob).item()\n",
        "                candidates.append((score + myitem / length_penalty_factor, new_seq, new_hidden))\n",
        "\n",
        "        # Select top-k candidates based on the accumulated scores\n",
        "        #beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n",
        "        # Define a custom comparison key function\n",
        "        def get_score(item):\n",
        "            return item[0]\n",
        "\n",
        "        # Use heapq.nlargest to retrieve the top candidates based on score\n",
        "        beam = heapq.nlargest(beam_width, candidates, key=get_score)\n",
        "\n",
        "        time_step += 1\n",
        "\n",
        "    # Select the best sequence from the beam as the output\n",
        "    #best_score, best_sequence, tempvar = max(beam, key=lambda x: x[0])\n",
        "    \n",
        "    best_score = float('-inf')\n",
        "    best_sequence = None\n",
        "    tempvar = None\n",
        "    for item in beam:\n",
        "        score = item[0]\n",
        "        sequence = item[1]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_sequence = sequence\n",
        "            tempvar = item[2]\n",
        "    \n",
        "    translated_word = ''\n",
        "    \n",
        "    for token in best_sequence[1:-1]:\n",
        "        translated_word += reverse_output_char_dict[token.item()]\n",
        "\n",
        "    return translated_word\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.886666Z",
          "iopub.execute_input": "2023-05-22T18:44:30.886921Z",
          "iopub.status.idle": "2023-05-22T18:44:30.900444Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.886899Z",
          "shell.execute_reply": "2023-05-22T18:44:30.899463Z"
        },
        "trusted": true,
        "id": "6hWgGu2B3W0z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up some initial parameters\n"
      ],
      "metadata": {
        "id": "sUoJhxUb3sPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "learning_rate = 0.001\n",
        "encoder_embedding_size = 256\n",
        "input_size_decoder = num_decoder_tokens\n",
        "batch_size = 32\n",
        "load_model = False\n",
        "num_enc_layers = 2\n",
        "input_size_encoder = num_encoder_tokens\n",
        "num_epochs = 2\n",
        "learning_rate = 0.001\n",
        "input_size_decoder = num_decoder_tokens\n",
        "batch_size = 32\n",
        "output_size = num_decoder_tokens\n",
        "\n",
        "decoder_embedding_size = 256\n",
        "hidden_size = 256  # Needs to be the same for both RNN's\n",
        "\n",
        "num_dec_layers = 2\n",
        "dropout=0.1\n",
        "training = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:30.911724Z",
          "iopub.execute_input": "2023-05-22T18:44:30.911971Z",
          "iopub.status.idle": "2023-05-22T18:44:30.919537Z",
          "shell.execute_reply.started": "2023-05-22T18:44:30.911950Z",
          "shell.execute_reply": "2023-05-22T18:44:30.918741Z"
        },
        "trusted": true,
        "id": "0i6avfy63W0z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_function(num_encoder_tokens,input_embedding_size, dp, cell_type, hidden_size, num_enc_layers, num_dec_layers,num_epochs,       output_size,    input_size_decoder,     batch_size,beam_width):\n",
        "    if(cell_type==\"LSTM\"):\n",
        "        #print(\"hello1\")\n",
        "        encoder_net = EncoderLSTM(input_size_encoder,input_embedding_size, hidden_size, num_enc_layers,dp).to(device)\n",
        "        #print(\"hello2\")\n",
        "        decoder_net = DecoderLSTM(input_size_decoder,input_embedding_size,hidden_size,output_size,num_dec_layers,dp).to(device)\n",
        "        #print(\"hello3\")\n",
        "        model = Seq2SeqLSTM(encoder_net, decoder_net).to(device)\n",
        "       # print(\"hello\")\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        train_ds_x = torch.split(input_data,batch_size,dim=1)\n",
        "        train_ds_y = torch.split(target_data,batch_size,dim=1)\n",
        "        #print(train_ds_x)\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "           # wandb.log({'epochs' : epoch})##############################\n",
        "            \n",
        "            total_loss=0#############################\n",
        "\n",
        "            model.eval()\n",
        "            model.train()\n",
        "\n",
        "            for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "        # Get input and targets and get to cuda\n",
        "                inp_data = x.to(device)\n",
        "                target = y.to(device)\n",
        "\n",
        "            # Forward prop\n",
        "                output = model(inp_data, target)\n",
        "\n",
        "\n",
        "                output = output[1:].reshape(-1, output.shape[2])\n",
        "                target = target[1:].reshape(-1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(output, target)\n",
        "                total_loss += loss#########################\n",
        "                \n",
        "\n",
        "            # Back prop\n",
        "                loss.backward()\n",
        "\n",
        "            # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "            # within a healthy range\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            # Gradient descent step\n",
        "                optimizer.step()\n",
        "                \n",
        "            ###################################################################################################################\n",
        "#             total_words = len(train_X)\n",
        "#             correct_pred = 0\n",
        "#             model.eval()\n",
        "#             for i in range(total_words):\n",
        "#        # print(val_Y[i][1:-1])\n",
        "#               #  decoded_sentence = translate(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "#                #           reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "#                #           num_encoder_tokens, num_decoder_tokens, device)\n",
        "#                 decoded_sentence = beam_search(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "#               reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "#               num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "#                 if train_Y[i][1:-1]== decoded_sentence:\n",
        "#                      correct_pred += 1\n",
        "#         #print(decoded_sentence)\n",
        "#         #print('\\n')\n",
        "#             train_accuracy = correct_pred / total_words\n",
        "            ######################################################################################################################\n",
        "            \n",
        "            \n",
        "            total_words = len(val_X)\n",
        "            correct_pred = 0\n",
        "            model.eval()\n",
        "            for i in range(total_words):\n",
        "                decoded_sentence = beam_searchLSTM(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "                      reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "                      num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "                \n",
        "                if val_Y[i][1:-1]== decoded_sentence:\n",
        "                     correct_pred += 1\n",
        "        #print(decoded_sentence)\n",
        "        #print('\\n')\n",
        "            test_accuracy = correct_pred / total_words\n",
        "\n",
        "            print(test_accuracy)\n",
        "            #########################################################################################################\n",
        "           # wandb.log({'val_accuracy' : test_accuracy*100})\n",
        "           # wandb.log({'train_loss' : total_loss})\n",
        "            #wandb.log({'train_accuracy' : train_accuracy*100})\n",
        "            ##########################################################################################################\n",
        "            \n",
        "            \n",
        "    elif(cell_type==\"GRU\"):\n",
        "            encoder_net = EncoderGRU(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "            decoder_net = DecoderGRU(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "            model = Seq2SeqGRU(encoder_net, decoder_net).to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            \n",
        "            \n",
        "            train_ds_x = torch.split(input_data,batch_size,dim=1)\n",
        "            train_ds_y = torch.split(target_data,batch_size,dim=1)\n",
        "            \n",
        "            for epoch in range(num_epochs):\n",
        "                print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "               # wandb.log({'epochs' : epoch})##############################\n",
        "            \n",
        "                total_loss=0#############################\n",
        "\n",
        "                model.eval()\n",
        "                model.train()\n",
        "                total_loss=0\n",
        "\n",
        "                for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "                    # Get input and targets and get to cuda\n",
        "                    inp_data = x.to(device)\n",
        "                    target = y.to(device)\n",
        "\n",
        "                    # Forward prop\n",
        "                    output = model(inp_data, target)\n",
        "\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "                    output = output[1:].reshape(-1, output.shape[2])\n",
        "                    target = target[1:].reshape(-1)\n",
        "                    \n",
        "    \n",
        "                    optimizer.zero_grad()\n",
        "                    loss = criterion(output, target)\n",
        "                    total_loss += loss#########################\n",
        "\n",
        "                    # Back prop\n",
        "                    loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "                    optimizer.step()\n",
        "            \n",
        "            \n",
        "            \n",
        "                ###################################################################################################################\n",
        "#                 total_words = len(train_X)\n",
        "#                 correct_pred = 0\n",
        "#                 model.eval()\n",
        "#                 for i in range(total_words):\n",
        "#            # print(val_Y[i][1:-1])\n",
        "#                   #  decoded_sentence = translate(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "#                    #           reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "#                    #           num_encoder_tokens, num_decoder_tokens, device)\n",
        "#                     decoded_sentence = beam_search(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "#                   reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "#                   num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "#                     if train_Y[i][1:-1]== decoded_sentence:\n",
        "#                          correct_pred += 1\n",
        "#             #print(decoded_sentence)\n",
        "#             #print('\\n')\n",
        "#                 train_accuracy = correct_pred / total_words\n",
        "            ######################################################################################################################\n",
        "                total_words = len(val_X)\n",
        "                correct_pred = 0\n",
        "                model.eval()\n",
        "                for i in range(total_words):\n",
        "       # print(val_Y[i][1:-1])\n",
        "                #    decoded_sentence = translateGR(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "                #              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "                #              num_encoder_tokens, num_decoder_tokens, device)\n",
        "                    decoded_sentence = beam_searchGRU_RNN(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "              num_encoder_tokens, num_decoder_tokens,beam_width,device)\n",
        "                    if val_Y[i][1:-1]== decoded_sentence:\n",
        "                         correct_pred += 1\n",
        "        #print(decoded_sentence)\n",
        "        #print('\\n')\n",
        "                test_accuracy = correct_pred / total_words\n",
        "\n",
        "                print(test_accuracy)\n",
        "               # wandb.log({'val_accuracy' : test_accuracy*100})\n",
        "               # wandb.log({'train_loss' : total_loss})\n",
        "                #wandb.log({'train_accuracy' : train_accuracy*100})\n",
        "                \n",
        "                \n",
        "                \n",
        "    else:\n",
        "            encoder_net = EncoderRNN(input_size_encoder, encoder_embedding_size, hidden_size, num_enc_layers, enc_dropout).to(device)\n",
        "            decoder_net = DecoderRNN(input_size_decoder,decoder_embedding_size,hidden_size,output_size,num_dec_layers,dec_dropout).to(device)\n",
        "            model = Seq2SeqRNN(encoder_net, decoder_net).to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            train_ds_x = torch.split(input_data,batch_size,dim=1)\n",
        "            train_ds_y = torch.split(target_data,batch_size,dim=1)\n",
        "            for epoch in range(num_epochs):\n",
        "                print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "               # wandb.log({'epochs' : epoch})##############################\n",
        "            \n",
        "                total_loss=0#############################\n",
        "\n",
        "                model.eval()\n",
        "                model.train()\n",
        "\n",
        "                for i, (x,y) in enumerate(zip(train_ds_x,train_ds_y)):\n",
        "                    # Get input and targets and get to cuda\n",
        "                    inp_data = x.to(device)\n",
        "                    target = y.to(device)\n",
        "\n",
        "                    # Forward prop\n",
        "                    output = model(inp_data, target)\n",
        "                    \n",
        "\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "                    output = output[1:].reshape(-1, output.shape[2])\n",
        "                    target = target[1:].reshape(-1)\n",
        "                    \n",
        "    \n",
        "                    optimizer.zero_grad()\n",
        "                    loss = criterion(output, target)\n",
        "                    total_loss += loss#########################\n",
        "\n",
        "                    # Back prop\n",
        "                    loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "                    optimizer.step()\n",
        "            \n",
        "                ###################################################################################################################\n",
        "#                 total_words = len(train_X)\n",
        "#                 correct_pred = 0\n",
        "#                 model.eval()\n",
        "#                 for i in range(total_words):\n",
        "#            # print(val_Y[i][1:-1])\n",
        "#                   #  decoded_sentence = translate(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "#                    #           reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "#                    #           num_encoder_tokens, num_decoder_tokens, device)\n",
        "#                     decoded_sentence = beam_search(model,train_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "#                   reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "#                   num_encoder_tokens, num_decoder_tokens,1,device)\n",
        "#                     if train_Y[i][1:-1]== decoded_sentence:\n",
        "#                          correct_pred += 1\n",
        "#             #print(decoded_sentence)\n",
        "#             #print('\\n')\n",
        "#                 train_accuracy = correct_pred / total_words\n",
        "            ######################################################################################################################\n",
        "                total_words = len(val_X)\n",
        "                correct_pred = 0\n",
        "                model.eval()\n",
        "                for i in range(total_words):\n",
        "       # print(val_Y[i][1:-1])\n",
        "                    #decoded_sentence = translateGR(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "                    #          reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "                    #          num_encoder_tokens, num_decoder_tokens, device)\n",
        "                    decoded_sentence = beam_searchGRU_RNN(model,val_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "              reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "              num_encoder_tokens, num_decoder_tokens,beam_width,device)\n",
        "                    if val_Y[i][1:-1]== decoded_sentence:\n",
        "                         correct_pred += 1\n",
        "        #print(decoded_sentence)\n",
        "        #print('\\n')\n",
        "                test_accuracy = correct_pred / total_words\n",
        "\n",
        "                print(test_accuracy)\n",
        "               wandb.log({'val_accuracy' : test_accuracy*100})\n",
        "                wandb.log({'train_loss' : total_loss})\n",
        "                #wandb.log({'train_accuracy' : train_accuracy*100}) \n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:31.511324Z",
          "iopub.execute_input": "2023-05-22T18:44:31.512012Z",
          "iopub.status.idle": "2023-05-22T18:44:31.551448Z",
          "shell.execute_reply.started": "2023-05-22T18:44:31.511982Z",
          "shell.execute_reply": "2023-05-22T18:44:31.550457Z"
        },
        "trusted": true,
        "id": "FzPIahMt3W0z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wnadb Training"
      ],
      "metadata": {
        "id": "Dtux2SQK5WgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
        "    'parameters': {'embedding_size': {'values': [128, 256, 512]},\n",
        "                   'hidden_size': {'values': [128, 256, 512]},\n",
        "                   'beam_search':{'values':[1,2,3,4,5]},\n",
        "                   'dropout': {'values': [0.1, 0.2, 0.3, 0.4]},\n",
        "                   'dec_num_layers':{'values': [1,2,3]},\n",
        "                   'batch_size': {'values': [128,256,512]},\n",
        "                   'cell_type': {'values': ['LSTM','GRU','RNN']},\n",
        "                   'epochs' :{'values':[10,20,30,40]},\n",
        "                   'enc_num_layers': {'values': [1,2,3]}\n",
        "                   \n",
        "                }}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:31.621301Z",
          "iopub.execute_input": "2023-05-22T18:44:31.621842Z",
          "iopub.status.idle": "2023-05-22T18:44:31.628996Z",
          "shell.execute_reply.started": "2023-05-22T18:44:31.621806Z",
          "shell.execute_reply": "2023-05-22T18:44:31.628190Z"
        },
        "trusted": true,
        "id": "tP9cSEBe3W00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func():\n",
        "    var1 = wandb.init()\n",
        "    var2 = var1.config\n",
        "    if(var2.cell_type==\"RNN\"):\n",
        "        var2.epochs = 10\n",
        "    training_function(input_size_encoder ,var2.embedding_size, var2.dropout, var2.cell_type, var2.hidden_size, var2.enc_num_layers,  var2.enc_num_layers,var2.epochs,num_decoder_tokens,num_decoder_tokens,var2.batch_size,var2.beam_search)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:33.917792Z",
          "iopub.execute_input": "2023-05-22T18:44:33.918172Z",
          "iopub.status.idle": "2023-05-22T18:44:33.925732Z",
          "shell.execute_reply.started": "2023-05-22T18:44:33.918140Z",
          "shell.execute_reply": "2023-05-22T18:44:33.924743Z"
        },
        "trusted": true,
        "id": "UVux7Abb3W00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key='f63ad1f9fa43fcd121f1958cdb98ba6cb4b8bcb1')\n",
        "wandb.init(project=\"cs6910_RNN\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:44:34.799327Z",
          "iopub.execute_input": "2023-05-22T18:44:34.799781Z",
          "iopub.status.idle": "2023-05-22T18:45:08.990916Z",
          "shell.execute_reply.started": "2023-05-22T18:44:34.799744Z",
          "shell.execute_reply": "2023-05-22T18:45:08.990105Z"
        },
        "trusted": true,
        "id": "DxSSr8qU3W00",
        "outputId": "114dc4ba-4de4-4974-e91e-620b26915614"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m017\u001b[0m (\u001b[33mcs22m017-anup\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_184437-l1j46xbq</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/l1j46xbq' target=\"_blank\">lilac-hill-38</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/l1j46xbq' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/l1j46xbq</a>"
          },
          "metadata": {}
        },
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/l1j46xbq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
            "text/plain": "<wandb.sdk.wandb_run.Run at 0x7a7bec091570>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"cs6910_RNN\")\n",
        "wandb.agent(sweep_id, train_func, count=50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-22T18:45:08.995197Z",
          "iopub.execute_input": "2023-05-22T18:45:08.997414Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "e54adbf22c0740d780399bbc4810eae1",
            "fb2e813fd14c42eeb51d81d84e941e67",
            "2970f9b66271461db6621c7e0f1f63e9",
            "a944e142855e49f8a93128b4b81c53b2",
            "fa54e52e8af54e14bbc4d8a6028d9cd0"
          ]
        },
        "id": "F2QhSM1G3W00",
        "outputId": "3bb89bc0-c521-42b1-d86d-589e55dd72ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Create sweep with ID: cxj54n1c\nSweep URL: https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "wandb: Waiting for W&B process to finish... (success).\nwandb: 🚀 View run lilac-hill-38 at: https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/l1j46xbq\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20230522_184437-l1j46xbq/logs\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dnaycogz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_184550-dnaycogz</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dnaycogz' target=\"_blank\">decent-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dnaycogz' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dnaycogz</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 10]\n0.0\n[Epoch 1 / 10]\n0.0\n[Epoch 2 / 10]\n0.000244140625\n[Epoch 3 / 10]\n0.0029296875\n[Epoch 4 / 10]\n0.052734375\n[Epoch 5 / 10]\n0.153564453125\n[Epoch 6 / 10]\n0.22216796875\n[Epoch 7 / 10]\n0.236328125\n[Epoch 8 / 10]\n0.26171875\n[Epoch 9 / 10]\n0.299560546875\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e54adbf22c0740d780399bbc4810eae1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▇▆▆▄▃▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▂▅▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>9</td></tr><tr><td>train_loss</td><td>37.89745</td></tr><tr><td>val_accuracy</td><td>29.95605</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">decent-sweep-1</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dnaycogz' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dnaycogz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_184550-dnaycogz/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m2jyyp8g with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_190459-m2jyyp8g</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/m2jyyp8g' target=\"_blank\">vocal-sweep-2</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/m2jyyp8g' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/m2jyyp8g</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 10]\n0.00244140625\n[Epoch 1 / 10]\n0.077392578125\n[Epoch 2 / 10]\n0.18505859375\n[Epoch 3 / 10]\n0.2451171875\n[Epoch 4 / 10]\n0.27197265625\n[Epoch 5 / 10]\n0.286376953125\n[Epoch 6 / 10]\n0.302978515625\n[Epoch 7 / 10]\n0.32470703125\n[Epoch 8 / 10]\n0.32666015625\n[Epoch 9 / 10]\n0.32861328125\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb2e813fd14c42eeb51d81d84e941e67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>9</td></tr><tr><td>train_loss</td><td>30.35181</td></tr><tr><td>val_accuracy</td><td>32.86133</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">vocal-sweep-2</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/m2jyyp8g' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/m2jyyp8g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_190459-m2jyyp8g/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qzsbx9y7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_192814-qzsbx9y7</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/qzsbx9y7' target=\"_blank\">zany-sweep-3</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/qzsbx9y7' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/qzsbx9y7</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 40]\n0.0\n[Epoch 1 / 40]\n0.000244140625\n[Epoch 2 / 40]\n0.0009765625\n[Epoch 3 / 40]\n0.004638671875\n[Epoch 4 / 40]\n0.011962890625\n[Epoch 5 / 40]\n0.018310546875\n[Epoch 6 / 40]\n0.0283203125\n[Epoch 7 / 40]\n0.03564453125\n[Epoch 8 / 40]\n0.052001953125\n[Epoch 9 / 40]\n0.058349609375\n[Epoch 10 / 40]\n0.060546875\n[Epoch 11 / 40]\n0.085205078125\n[Epoch 12 / 40]\n0.07861328125\n[Epoch 13 / 40]\n0.09716796875\n[Epoch 14 / 40]\n0.084716796875\n[Epoch 15 / 40]\n0.119384765625\n[Epoch 16 / 40]\n0.114013671875\n[Epoch 17 / 40]\n0.120361328125\n[Epoch 18 / 40]\n0.134765625\n[Epoch 19 / 40]\n0.136962890625\n[Epoch 20 / 40]\n0.153564453125\n[Epoch 21 / 40]\n0.15625\n[Epoch 22 / 40]\n0.156982421875\n[Epoch 23 / 40]\n0.1611328125\n[Epoch 24 / 40]\n0.170654296875\n[Epoch 25 / 40]\n0.173095703125\n[Epoch 26 / 40]\n0.1826171875\n[Epoch 27 / 40]\n0.171630859375\n[Epoch 28 / 40]\n0.184326171875\n[Epoch 29 / 40]\n0.19189453125\n[Epoch 30 / 40]\n0.19921875\n[Epoch 31 / 40]\n0.199951171875\n[Epoch 32 / 40]\n0.196044921875\n[Epoch 33 / 40]\n0.203857421875\n[Epoch 34 / 40]\n0.2021484375\n[Epoch 35 / 40]\n0.19482421875\n[Epoch 36 / 40]\n0.20947265625\n[Epoch 37 / 40]\n0.208984375\n[Epoch 38 / 40]\n0.2265625\n[Epoch 39 / 40]\n0.2236328125\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▂▂▂▃▃▃▄▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>39</td></tr><tr><td>train_loss</td><td>95.86891</td></tr><tr><td>val_accuracy</td><td>22.36328</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">zany-sweep-3</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/qzsbx9y7' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/qzsbx9y7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_192814-qzsbx9y7/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s3o6ssgb with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_200941-s3o6ssgb</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/s3o6ssgb' target=\"_blank\">vocal-sweep-4</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/s3o6ssgb' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/s3o6ssgb</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 10]\n0.0\n[Epoch 1 / 10]\n0.00244140625\n[Epoch 2 / 10]\n0.032470703125\n[Epoch 3 / 10]\n0.0595703125\n[Epoch 4 / 10]\n0.121337890625\n[Epoch 5 / 10]\n0.168212890625\n[Epoch 6 / 10]\n0.2080078125\n[Epoch 7 / 10]\n0.215087890625\n[Epoch 8 / 10]\n0.236083984375\n[Epoch 9 / 10]\n0.24658203125\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2970f9b66271461db6621c7e0f1f63e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▂▃▄▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>9</td></tr><tr><td>train_loss</td><td>40.97554</td></tr><tr><td>val_accuracy</td><td>24.6582</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">vocal-sweep-4</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/s3o6ssgb' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/s3o6ssgb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_200941-s3o6ssgb/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mpj1zzp1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_202600-mpj1zzp1</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/mpj1zzp1' target=\"_blank\">breezy-sweep-5</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/mpj1zzp1' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/mpj1zzp1</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 40]\n0.0009765625\n[Epoch 1 / 40]\n0.07470703125\n[Epoch 2 / 40]\n0.13232421875\n[Epoch 3 / 40]\n0.1640625\n[Epoch 4 / 40]\n0.189208984375\n[Epoch 5 / 40]\n0.2353515625\n[Epoch 6 / 40]\n0.25048828125\n[Epoch 7 / 40]\n0.2529296875\n[Epoch 8 / 40]\n0.27587890625\n[Epoch 9 / 40]\n0.285400390625\n[Epoch 10 / 40]\n0.288818359375\n[Epoch 11 / 40]\n0.301025390625\n[Epoch 12 / 40]\n0.317138671875\n[Epoch 13 / 40]\n0.308349609375\n[Epoch 14 / 40]\n0.313720703125\n[Epoch 15 / 40]\n0.330322265625\n[Epoch 16 / 40]\n0.3232421875\n[Epoch 17 / 40]\n0.337890625\n[Epoch 18 / 40]\n0.335205078125\n[Epoch 19 / 40]\n0.34228515625\n[Epoch 20 / 40]\n0.338623046875\n[Epoch 21 / 40]\n0.328125\n[Epoch 22 / 40]\n0.326416015625\n[Epoch 23 / 40]\n0.328369140625\n[Epoch 24 / 40]\n0.33203125\n[Epoch 25 / 40]\n0.332763671875\n[Epoch 26 / 40]\n0.33447265625\n[Epoch 27 / 40]\n0.329345703125\n[Epoch 28 / 40]\n0.325927734375\n[Epoch 29 / 40]\n0.341796875\n[Epoch 30 / 40]\n0.34326171875\n[Epoch 31 / 40]\n0.343994140625\n[Epoch 32 / 40]\n0.344482421875\n[Epoch 33 / 40]\n0.336669921875\n[Epoch 34 / 40]\n0.341552734375\n[Epoch 35 / 40]\n0.333251953125\n[Epoch 36 / 40]\n0.337646484375\n[Epoch 37 / 40]\n0.3310546875\n[Epoch 38 / 40]\n0.34130859375\n[Epoch 39 / 40]\n0.334716796875\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a944e142855e49f8a93128b4b81c53b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇▇▇█████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>39</td></tr><tr><td>train_loss</td><td>65.24144</td></tr><tr><td>val_accuracy</td><td>33.47168</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">breezy-sweep-5</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/mpj1zzp1' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/mpj1zzp1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_202600-mpj1zzp1/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dgmc7czd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_213746-dgmc7czd</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dgmc7czd' target=\"_blank\">good-sweep-6</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dgmc7czd' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dgmc7czd</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 30]\n0.0\n[Epoch 1 / 30]\n0.02001953125\n[Epoch 2 / 30]\n0.14306640625\n[Epoch 3 / 30]\n0.22314453125\n[Epoch 4 / 30]\n0.2412109375\n[Epoch 5 / 30]\n0.2783203125\n[Epoch 6 / 30]\n0.298095703125\n[Epoch 7 / 30]\n0.3154296875\n[Epoch 8 / 30]\n0.329345703125\n[Epoch 9 / 30]\n0.3359375\n[Epoch 10 / 30]\n0.3466796875\n[Epoch 11 / 30]\n0.351806640625\n[Epoch 12 / 30]\n0.34326171875\n[Epoch 13 / 30]\n0.351318359375\n[Epoch 14 / 30]\n0.350830078125\n[Epoch 15 / 30]\n0.346923828125\n[Epoch 16 / 30]\n0.338623046875\n[Epoch 17 / 30]\n0.34033203125\n[Epoch 18 / 30]\n0.3330078125\n[Epoch 19 / 30]\n0.3447265625\n[Epoch 20 / 30]\n0.34716796875\n[Epoch 21 / 30]\n0.348388671875\n[Epoch 22 / 30]\n0.35009765625\n[Epoch 23 / 30]\n0.344482421875\n[Epoch 24 / 30]\n0.3447265625\n[Epoch 25 / 30]\n0.345458984375\n[Epoch 26 / 30]\n0.347900390625\n[Epoch 27 / 30]\n0.357666015625\n[Epoch 28 / 30]\n0.355712890625\n[Epoch 29 / 30]\n0.36669921875\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa54e52e8af54e14bbc4d8a6028d9cd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▄▅▆▆▇▇▇▇██████▇▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>29</td></tr><tr><td>train_loss</td><td>10.6684</td></tr><tr><td>val_accuracy</td><td>36.66992</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">good-sweep-6</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dgmc7czd' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/dgmc7czd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_213746-dgmc7czd/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5e9l7fo1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230522_224323-5e9l7fo1</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/5e9l7fo1' target=\"_blank\">lively-sweep-7</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/5e9l7fo1' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/5e9l7fo1</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 30]\n0.001220703125\n[Epoch 1 / 30]\n0.050048828125\n[Epoch 2 / 30]\n0.164794921875\n[Epoch 3 / 30]\n0.219482421875\n[Epoch 4 / 30]\n0.218505859375\n[Epoch 5 / 30]\n0.289306640625\n[Epoch 6 / 30]\n0.30859375\n[Epoch 7 / 30]\n0.32275390625\n[Epoch 8 / 30]\n0.343994140625\n[Epoch 9 / 30]\n0.341552734375\n[Epoch 10 / 30]\n0.348388671875\n[Epoch 11 / 30]\n0.36083984375\n[Epoch 12 / 30]\n0.352294921875\n[Epoch 13 / 30]\n0.355224609375\n[Epoch 14 / 30]\n0.347412109375\n[Epoch 15 / 30]\n0.343994140625\n[Epoch 16 / 30]\n0.381103515625\n[Epoch 17 / 30]\n0.373291015625\n[Epoch 18 / 30]\n0.351318359375\n[Epoch 19 / 30]\n0.369140625\n[Epoch 20 / 30]\n0.367919921875\n[Epoch 21 / 30]\n0.369140625\n[Epoch 22 / 30]\n0.364990234375\n[Epoch 23 / 30]\n0.36279296875\n[Epoch 24 / 30]\n0.361572265625\n[Epoch 25 / 30]\n0.36279296875\n[Epoch 26 / 30]\n0.382080078125\n[Epoch 27 / 30]\n0.375244140625\n[Epoch 28 / 30]\n0.366455078125\n[Epoch 29 / 30]\n0.34326171875\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▅▅▆▇▇▇▇▇█▇█▇▇██▇██████████▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>29</td></tr><tr><td>train_loss</td><td>8.37255</td></tr><tr><td>val_accuracy</td><td>34.32617</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">lively-sweep-7</strong> at: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/5e9l7fo1' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/5e9l7fo1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230522_224323-5e9l7fo1/logs</code>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cn0q52jf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_num_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230523_013648-cn0q52jf</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/cn0q52jf' target=\"_blank\">rose-sweep-8</a></strong> to <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/sweeps/cxj54n1c</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/cn0q52jf' target=\"_blank\">https://wandb.ai/cs22m017-anup/cs6910_RNN/runs/cn0q52jf</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[Epoch 0 / 40]\n0.000244140625\n[Epoch 1 / 40]\n0.00146484375\n[Epoch 2 / 40]\n0.018798828125\n[Epoch 3 / 40]\n0.06640625\n[Epoch 4 / 40]\n0.0947265625\n[Epoch 5 / 40]\n0.154052734375\n[Epoch 6 / 40]\n0.202392578125\n[Epoch 7 / 40]\n0.21240234375\n[Epoch 8 / 40]\n0.243408203125\n[Epoch 9 / 40]\n0.233154296875\n[Epoch 10 / 40]\n0.27978515625\n[Epoch 11 / 40]\n0.290283203125\n[Epoch 12 / 40]\n0.279052734375\n[Epoch 13 / 40]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training on the best hyperparameters"
      ],
      "metadata": {
        "id": "-y6jlT0K4NAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=training(input_size_encoder ,512, 0.4, 'GRU', 512, 3,   3,40,num_decoder_tokens,num_decoder_tokens,512,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFIUT6fV3W01",
        "outputId": "32ed43e6-72e8-4b9f-ee98-16ff7ce111de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 40]\n",
            "0.001220703125\n",
            "[Epoch 1 / 40]\n",
            "0.033203125\n",
            "[Epoch 2 / 40]\n",
            "0.1376953125\n",
            "[Epoch 3 / 40]\n",
            "0.196533203125\n",
            "[Epoch 4 / 40]\n",
            "0.268310546875\n",
            "[Epoch 5 / 40]\n",
            "0.276611328125\n",
            "[Epoch 6 / 40]\n",
            "0.29443359375\n",
            "[Epoch 7 / 40]\n",
            "0.314453125\n",
            "[Epoch 8 / 40]\n",
            "0.322998046875\n",
            "[Epoch 9 / 40]\n",
            "0.34716796875\n",
            "[Epoch 10 / 40]\n",
            "0.34912109375\n",
            "[Epoch 11 / 40]\n",
            "0.35693359375\n",
            "[Epoch 12 / 40]\n",
            "0.333251953125\n",
            "[Epoch 13 / 40]\n",
            "0.35107421875\n",
            "[Epoch 14 / 40]\n",
            "0.35595703125\n",
            "[Epoch 15 / 40]\n",
            "0.332275390625\n",
            "[Epoch 16 / 40]\n",
            "0.35693359375\n",
            "[Epoch 17 / 40]\n",
            "0.358154296875\n",
            "[Epoch 18 / 40]\n",
            "0.364501953125\n",
            "[Epoch 19 / 40]\n",
            "0.339599609375\n",
            "[Epoch 20 / 40]\n",
            "0.361328125\n",
            "[Epoch 21 / 40]\n",
            "0.371826171875\n",
            "[Epoch 22 / 40]\n",
            "0.372802734375\n",
            "[Epoch 23 / 40]\n",
            "0.373046875\n",
            "[Epoch 24 / 40]\n",
            "0.37060546875\n",
            "[Epoch 25 / 40]\n",
            "0.357421875\n",
            "[Epoch 26 / 40]\n",
            "0.354248046875\n",
            "[Epoch 27 / 40]\n",
            "0.353759765625\n",
            "[Epoch 28 / 40]\n",
            "0.353271484375\n",
            "[Epoch 29 / 40]\n",
            "0.361328125\n",
            "[Epoch 30 / 40]\n",
            "0.348876953125\n",
            "[Epoch 31 / 40]\n",
            "0.3291015625\n",
            "[Epoch 32 / 40]\n",
            "0.354736328125\n",
            "[Epoch 33 / 40]\n",
            "0.36572265625\n",
            "[Epoch 34 / 40]\n",
            "0.3603515625\n",
            "[Epoch 35 / 40]\n",
            "0.37255859375\n",
            "[Epoch 36 / 40]\n",
            "0.369140625\n",
            "[Epoch 37 / 40]\n",
            "0.367431640625\n",
            "[Epoch 38 / 40]\n",
            "0.37255859375\n",
            "[Epoch 39 / 40]\n",
            "0.36376953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtZFp5RmBHQW",
        "outputId": "cd7e7165-ae2b-477d-90c1-1659825a2558"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqGR(\n",
              "  (encoder): EncoderGRU(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (embedding): Embedding(27, 256)\n",
              "    (rnn): GRU(256, 512, num_layers=3, dropout=0.1)\n",
              "  )\n",
              "  (decoder): DecoderGRU(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (embedding): Embedding(67, 256)\n",
              "    (rnn): GRU(256, 512, num_layers=3, dropout=0.1)\n",
              "    (fc): Linear(in_features=512, out_features=67, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#writing prediction on the file"
      ],
      "metadata": {
        "id": "TxQlcPoQdzw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_list_to_file(string_list, file_path):\n",
        "    with open(file_path, \"a\") as file:\n",
        "        line = ' '.join(string_list)  # Join the strings with spaces\n",
        "        file.write(line + \"\\n\")  # Append a newline character at the end\n"
      ],
      "metadata": {
        "id": "BW4TT-dn3W01"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "def beam_searchGRU_RNN(model, word, input_char_index, output_char_index, reverse_input_char_index,\n",
        "                reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length,\n",
        "                num_encoder_tokens, num_decoder_tokens, beam_width, device, length_penalty=0.6):\n",
        "\n",
        "    word_t = ''\n",
        "\n",
        "    # Encode the input word\n",
        "    data = np.zeros((max_encoder_seq_length, 1), dtype=\"int64\")\n",
        "    # for t, char in enumerate(word):\n",
        "    #     if char in input_char_index:\n",
        "    #         data[t, 0] = input_char_index[char]\n",
        "    #     else:\n",
        "    #         data[t,0]=69\n",
        "\n",
        "\n",
        "    for t, char in enumerate(word):\n",
        "        if char in input_char_index:\n",
        "            if t < data.shape[0]:\n",
        "                data[t, 0] = input_char_index[char]\n",
        "        else:\n",
        "            if t < data.shape[0]:\n",
        "                data[t, 0] = 69\n",
        "\n",
        "    data[t + 1:, 0] = input_char_index[\" \"]\n",
        "\n",
        "    data = torch.tensor(data, dtype=torch.int64).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(data)\n",
        "\n",
        "    # Initialize beam\n",
        "    initial_sequence = torch.tensor(np.array(output_char_index['\\t']).reshape(1,)).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "        candidates = []\n",
        "        for score, seq, hidden in beam:\n",
        "            last_token = seq[-1].item()\n",
        "            if last_token == output_char_index['\\n']:\n",
        "                # If the sequence ends with the end token, add it to the candidates\n",
        "                candidates.append((score, seq, hidden))\n",
        "                continue\n",
        "\n",
        "            x = torch.tensor(np.array(last_token).reshape(1,)).to(device)\n",
        "            output, hidden = model.decoder(x, hidden.squeeze(0))\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "            # Get the top-k probabilities and tokens\n",
        "            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n",
        "\n",
        "            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "                new_hidden = hidden.clone().unsqueeze(0)\n",
        "                length_penalty_factor = ((len(new_seq) - 1) / 5) ** length_penalty  # Adjust penalty factor as needed\n",
        "                candidates.append((score + torch.log(prob).item() / length_penalty_factor, new_seq, new_hidden))\n",
        "\n",
        "        # Select top-k candidates based on the accumulated scores\n",
        "        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n",
        "\n",
        "    # Select the best sequence from the beam as the output\n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])\n",
        "    word_t = ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:-1]])\n",
        "\n",
        "    return word_t"
      ],
      "metadata": {
        "id": "C-iwTXgX4g30"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(test_X)\n",
        "correct_pred = 0\n",
        "model.eval()\n",
        "for i in range(total_words):\n",
        "\n",
        "    decoded_sentence = beam_searchGR(model,test_X[i], input_char_index, output_char_index, reverse_input_char_index, \n",
        "    reverse_target_char_index, max_encoder_seq_length, max_decoder_seq_length, \n",
        "    num_encoder_tokens, num_decoder_tokens,2,device)\n",
        "    str1= \"input sequence      \" + test_X[i]  + \"\\n\"\n",
        "    str2=  \"actual output       \"+test_Y[i]\n",
        "    str3= \"predicted output    \"+decoded_sentence+\"\\n\\n\"\n",
        "    str=[str1,str2,str3]\n",
        "    if i%500==0:\n",
        "        print(str1,str2,str3)\n",
        "    write_list_to_file(str,\"/content/total.txt\")\n",
        "    if test_Y[i][1:-1]== decoded_sentence:\n",
        "        correct_pred += 1\n",
        "        write_list_to_file(str,\"/content/correct.txt\")\n",
        "    else :\n",
        "        write_list_to_file(str,\"/content/wrong.txt\")\n",
        "\n",
        "\n",
        "#print(decoded_sentence)\n",
        "#print('\\n')\n",
        "test_accuracy = correct_pred / total_words\n",
        "\n",
        "test_accuracy = correct_pred / total_words\n",
        "print(\"total number of words:\",total_words)\n",
        "print(\"correct number of words:\",correct_pred)\n",
        "print(\"Test Accuracy is :\",test_accuracy)\n",
        "\n",
        "print(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz4vU01P4lpq",
        "outputId": "e5626661-fd96-42c4-a9a7-b5f615404e2f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sequence      thermax\n",
            " actual output       \tथरमैक्स\n",
            " predicted output    थररमैक्स\n",
            "\n",
            "\n",
            "input sequence      deepika\n",
            " actual output       \tदीपिका\n",
            " predicted output    दीपिका\n",
            "\n",
            "\n",
            "input sequence      supachye\n",
            " actual output       \tसुपाच्य\n",
            " predicted output    सुपा्यय\n",
            "\n",
            "\n",
            "input sequence      depthiriya\n",
            " actual output       \tडिप्थीरिया\n",
            " predicted output    देपीथिरया\n",
            "\n",
            "\n",
            "input sequence      insano\n",
            " actual output       \tइंसानो\n",
            " predicted output    इंसानो\n",
            "\n",
            "\n",
            "input sequence      menrajya\n",
            " actual output       \tमेंराज्य\n",
            " predicted output    मेनराज्य\n",
            "\n",
            "\n",
            "input sequence      gujarana\n",
            " actual output       \tगुजारना\n",
            " predicted output    गुजररना\n",
            "\n",
            "\n",
            "input sequence      wokha\n",
            " actual output       \tवोखा\n",
            " predicted output    वोखा\n",
            "\n",
            "\n",
            "input sequence      chaurasiya\n",
            " actual output       \tचौरसिया\n",
            " predicted output    चौरसिया\n",
            "\n",
            "\n",
            "total number of words: 4096\n",
            "correct number of words: 1426\n",
            "Test Accuracy is : 0.34814453125\n",
            "0.34814453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfhSu3upHhAK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}